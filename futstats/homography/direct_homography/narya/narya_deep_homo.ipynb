{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_layer(\n",
    "    x, indx, activation=\"tanh\", output_size=8, nb_neurons=[512, 512, 256, 128]\n",
    "):\n",
    "    \"\"\"Fully connected layers to add at the end of a network.\n",
    "\n",
    "    Arguments:\n",
    "        x: a tf.keras Tensor as input\n",
    "        indx: Integer, an index to add to the name of the layers\n",
    "        activation: String, name of the activation function to add at the end\n",
    "        output_size: Size of the last layer, number of outputs\n",
    "        nb_neurons: Size of the Dense layer to add\n",
    "    Returns:\n",
    "        output: a tf.keras Tensor as output\n",
    "    Raises:\n",
    "\n",
    "    \"\"\"\n",
    "    dense_name_base = \"full_\" + str(indx)\n",
    "    for indx, neuron in enumerate(nb_neurons):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            neuron, name=dense_name_base + str(neuron) + \"_\" + str(indx)\n",
    "        )(x)\n",
    "    x = tf.keras.layers.Dense(output_size, name=dense_name_base + \"output\")(x)\n",
    "    output = tf.keras.layers.Activation(activation)(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET_ARCHI_TF_KERAS_PATH = (\n",
    "    \"https://storage.googleapis.com/narya-bucket-1/models/deep_homo_model_1.h5\"\n",
    ")\n",
    "RESNET_ARCHI_TF_KERAS_NAME = \"deep_homo_model_1.h5\"\n",
    "RESNET_ARCHI_TF_KERAS_TOTAR = False\n",
    "\n",
    "\n",
    "\n",
    "def _build_resnet18():\n",
    "    \"\"\"Builds a resnet18 model in keras from a .h5 file.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        a tf.keras.models.Model\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "    resnet18_path_to_file = \"./deep_homo_model_1.h5\"\n",
    "    # resnet18_path_to_file = tf.keras.utils.get_file(\n",
    "    #     RESNET_ARCHI_TF_KERAS_NAME,\n",
    "    #     RESNET_ARCHI_TF_KERAS_PATH,\n",
    "    #     RESNET_ARCHI_TF_KERAS_TOTAR,\n",
    "    # )\n",
    "\n",
    "    resnet18 = tf.keras.models.load_model(resnet18_path_to_file)\n",
    "    resnet18.compile()\n",
    "\n",
    "    inputs = resnet18.input\n",
    "    outputs = resnet18.layers[-2].output\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"custom_resnet18\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_img_to_np_img(torch_img):\n",
    "    \"\"\"Convert a torch image to a numpy image\n",
    "\n",
    "    Arguments:\n",
    "        torch_img: Tensor of shape (B,C,H,W) or (C,H,W)\n",
    "    Returns:\n",
    "        a np.array of shape (B,H,W,C) or (H,W,C)\n",
    "    Raises:\n",
    "        ValueError: If this is not a Torch tensor\n",
    "    \"\"\"\n",
    "    if isinstance(torch_img, np.ndarray):\n",
    "        return torch_img\n",
    "    assert isinstance(torch_img, torch.Tensor), \"cannot process data type: {0}\".format(\n",
    "        type(torch_img)\n",
    "    )\n",
    "    if len(torch_img.shape) == 4 and (\n",
    "        torch_img.shape[1] == 3 or torch_img.shape[1] == 1\n",
    "    ):\n",
    "        return np.transpose(torch_img.detach().cpu().numpy(), (0, 2, 3, 1))\n",
    "    if len(torch_img.shape) == 3 and (\n",
    "        torch_img.shape[0] == 3 or torch_img.shape[0] == 1\n",
    "    ):\n",
    "        return np.transpose(torch_img.detach().cpu().numpy(), (1, 2, 0))\n",
    "    elif len(torch_img.shape) == 2:\n",
    "        return torch_img.detach().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"cannot process this image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(np_array):\n",
    "    \"\"\"Parse a numpy array to a torch variable\n",
    "\n",
    "    Arguments:\n",
    "        np_array: a np.array \n",
    "    Returns:\n",
    "        a torch Var with the same value as the np_array\n",
    "    Raises:\n",
    "        \n",
    "    \"\"\"\n",
    "    tensor = torch.from_numpy(np_array).float()\n",
    "    return torch.autograd.Variable(tensor, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_img_to_torch_img(np_img):\n",
    "    \"\"\"Convert a np image to a torch image\n",
    "\n",
    "    Arguments:\n",
    "        np_img: a np.array of shape (B,H,W,C) or (H,W,C)\n",
    "    Returns:\n",
    "        a Tensor of shape (B,C,H,W) or (C,H,W)\n",
    "    Raises:\n",
    "        ValueError: If this is not a np.array\n",
    "    \"\"\"\n",
    "    if isinstance(np_img, torch.Tensor):\n",
    "        return np_img\n",
    "    assert isinstance(np_img, np.ndarray), \"cannot process data type: {0}\".format(\n",
    "        type(np_img)\n",
    "    )\n",
    "    if len(np_img.shape) == 4 and (np_img.shape[3] == 3 or np_img.shape[3] == 1):\n",
    "        return to_torch(np.transpose(np_img, (0, 3, 1, 2)))\n",
    "    if len(np_img.shape) == 3 and (np_img.shape[2] == 3 or np_img.shape[2] == 1):\n",
    "        return to_torch(np.transpose(np_img, (2, 0, 1)))\n",
    "    elif len(np_img.shape) == 2:\n",
    "        return to_torch(np_img)\n",
    "    else:\n",
    "        raise ValueError(\"cannot process this image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_single_image_torch(image, img_mean=None, img_std=None):\n",
    "    \"\"\"Normalize a Torch tensor\n",
    "\n",
    "    Arguments:\n",
    "        image: Torch Tensor of shape (C,W,H)\n",
    "        img_mean: List of mean per channel (e.g.: [0.485, 0.456, 0.406])\n",
    "        img_std: List of std per channel (e.g.: [0.229, 0.224, 0.225])\n",
    "    Returns:\n",
    "        image: Torch Tensor of shape (C,W,H), the normalized image\n",
    "    Raises:\n",
    "        ValueError: If the shape of the image is not of lenth 3\n",
    "        ValueError: If the image is not a torch Tensor\n",
    "    \"\"\"\n",
    "    if len(image.shape) != 3:\n",
    "        raise ValueError(\n",
    "            \"The len(shape) of the image is {}, not 3\".format(len(image.shape))\n",
    "        )\n",
    "    if isinstance(image, torch.Tensor) == False:\n",
    "        raise ValueError(\"The image is not a torch Tensor\")\n",
    "    if img_mean is None and img_std is None:\n",
    "        img_mean = torch.mean(image, dim=(1, 2)).view(-1, 1, 1)\n",
    "        img_std = image.contiguous().view(image.size(0), -1).std(-1).view(-1, 1, 1)\n",
    "        image = (image - img_mean) / img_std\n",
    "    else:\n",
    "        image = Normalize(img_mean, img_std, inplace=False)(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_homo_preprocessing(input_shape):\n",
    "    \"\"\"Builds the preprocessing function for the Deep Homography estimation Model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocessing(input_img, **kwargs):\n",
    "\n",
    "        if len(input_img.shape) == 4:\n",
    "            print(\n",
    "                \"Only preprocessing single image, we will consider the first one of the batch\"\n",
    "            )\n",
    "            image = input_img[0]\n",
    "        else:\n",
    "            image = input_img\n",
    "\n",
    "        image = cv2.resize(image, input_shape)\n",
    "        image = torch_img_to_np_img(\n",
    "            normalize_single_image_torch(np_img_to_torch_img(image))\n",
    "        )\n",
    "        return image\n",
    "\n",
    "    return preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepHomoModel:\n",
    "    \"\"\"Class for Keras Models to predict the corners displacement from an image. These corners can then get used\n",
    "    to compute the homography.\n",
    "\n",
    "    Arguments:\n",
    "        pretrained: Boolean, if the model is loaded pretrained on ImageNet or not\n",
    "        input_shape: Tuple, shape of the model's input\n",
    "    Call arguments:\n",
    "        input_img: a np.array of shape input_shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=False, input_shape=(256, 256)):\n",
    "        self.input_shape = input_shape\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "        self.resnet_18 = _build_resnet18()\n",
    "\n",
    "        inputs = tf.keras.layers.Input((self.input_shape[0], self.input_shape[1], 3))\n",
    "        x = self.resnet_18(inputs)\n",
    "        outputs = pyramid_layer(x, 2)\n",
    "\n",
    "        self.model = tf.keras.models.Model(\n",
    "            inputs=[inputs], outputs=outputs, name=\"DeepHomoPyramidalFull\"\n",
    "        )\n",
    "\n",
    "        self.preprocessing = _build_homo_preprocessing(input_shape)\n",
    "\n",
    "    def __call__(self, input_img):\n",
    "        img = self.preprocessing(input_img)\n",
    "        corners = self.model.predict(np.array([img]))\n",
    "\n",
    "        return corners\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        try:\n",
    "            self.model.load_weights(weights_path)\n",
    "            print(\"Succesfully loaded weights from {}\".format(weights_path))\n",
    "        except:\n",
    "            orig_weights = \"Randomly\"\n",
    "            print(\n",
    "                \"Could not load weights from {}, weights will be loaded {}\".format(\n",
    "                    weights_path, orig_weights\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad marshal data (unknown type code)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m direct_homography_model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepHomoModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m direct_homography_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./deep_homo_model_wei.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m, in \u001b[0;36mDeepHomoModel.__init__\u001b[0;34m(self, pretrained, input_shape)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained \u001b[38;5;241m=\u001b[39m pretrained\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet_18 \u001b[38;5;241m=\u001b[39m \u001b[43m_build_resnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet_18(inputs)\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36m_build_resnet18\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m resnet18_path_to_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./deep_homo_model_1.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# resnet18_path_to_file = tf.keras.utils.get_file(\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     RESNET_ARCHI_TF_KERAS_NAME,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     RESNET_ARCHI_TF_KERAS_PATH,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     RESNET_ARCHI_TF_KERAS_TOTAR,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m resnet18 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18_path_to_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m resnet18\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m     28\u001b[0m inputs \u001b[38;5;241m=\u001b[39m resnet18\u001b[38;5;241m.\u001b[39minput\n",
      "File \u001b[0;32m~/miniconda3/envs/futstats-tf/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/futstats-tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/futstats-tf/lib/python3.10/site-packages/keras/src/utils/generic_utils.py:102\u001b[0m, in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m, binascii\u001b[38;5;241m.\u001b[39mError):\n\u001b[1;32m    101\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_unicode_escape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m globs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     globs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: bad marshal data (unknown type code)"
     ]
    }
   ],
   "source": [
    "direct_homography_model = DeepHomoModel()\n",
    "\n",
    "direct_homography_model.load_weights(\"./deep_homo_model_wei.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "futstats-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
